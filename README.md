# IR Spectroscopy ML Project: Classifying Organic Molecules

A comprehensive machine learning pipeline for analyzing Infrared (IR) spectroscopy data to classify organic molecules based on their functional groups.

## üéØ Project Overview

This project implements a complete end-to-end pipeline that:
- Preprocesses raw IR spectral data with advanced signal processing techniques
- Extracts features from 31 key functional group regions
- Trains and evaluates 8 different machine learning models
- Performs multi-label classification to identify functional groups
- Provides detailed performance analysis and visualizations

## üìÅ Project Structure

```
ML_Project_Classifying_Organic_Molecule/
‚îú‚îÄ‚îÄ ML_Project_CS390_CS394.ipynb    # Main refactored notebook (58 cells)
‚îú‚îÄ‚îÄ requirements.txt                 # Python dependencies with versions
‚îú‚îÄ‚îÄ README.md                        # This comprehensive guide
‚îú‚îÄ‚îÄ REFACTORING_SUMMARY.md          # Detailed refactoring documentation
‚îú‚îÄ‚îÄ Test Data.csv                    # Input IR spectroscopy data
‚îÇ
‚îî‚îÄ‚îÄ Generated Output Files:
    ‚îú‚îÄ‚îÄ preprocessed_spectrum.csv        # Cleaned and processed spectrum
    ‚îú‚îÄ‚îÄ resampled_spectrum.csv           # Resampled to 500 uniform points
    ‚îú‚îÄ‚îÄ extracted_features.csv           # 124 features per spectrum
    ‚îú‚îÄ‚îÄ model_comparison.csv             # Performance metrics for 8 models
    ‚îú‚îÄ‚îÄ functional_group_performance.csv # Per-group classification metrics
    ‚îî‚îÄ‚îÄ predictions.csv                  # Best model predictions
```

### File Descriptions

**Core Files**
- `ML_Project_CS390_CS394.ipynb` - Main analysis notebook with 58 cells organized into 2 parts
- `requirements.txt` - Complete dependency list with exact versions
- `Test Data.csv` - Input IR spectroscopy data (wavenumber vs intensity)

**Documentation**
- `README.md` - Complete project documentation (this file)
- `REFACTORING_SUMMARY.md` - Details about code improvements and structure

**Generated Outputs** (created when running the notebook)
- All CSV files are automatically generated and saved to project root
- Can be regenerated by running the respective cells

## üöÄ Getting Started

### Prerequisites

- Python 3.12+ (tested with Python 3.12.3)
- Jupyter Notebook or VS Code with Jupyter extension
- 2GB RAM minimum (recommended: 4GB for faster processing)

### Installation

1. **Clone or download the repository**
   ```bash
   cd ML_Project_Classifying_Organic_Molecule
   ```

2. **Create a virtual environment** (recommended)
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -r requirements.txt
   ```
   This will install all required packages including numpy, pandas, scikit-learn, matplotlib, seaborn, and scipy.

### Running the Notebook

**Option 1: VS Code (Recommended)**
1. Open VS Code in the project directory
2. Install the "Jupyter" extension if not already installed
3. Open `ML_Project_CS390_CS394.ipynb`
4. Click "Run All" or execute cells individually

**Option 2: Jupyter Notebook**
1. Launch Jupyter in the terminal:
   ```bash
   jupyter notebook ML_Project_CS390_CS394.ipynb
   ```
2. Run all cells in order (Cell ‚Üí Run All) or execute step-by-step

**Option 3: JupyterLab**
   ```bash
   jupyter lab
   ```
   Then navigate to the notebook and run cells

### Execution Options

- **Full Pipeline**: Run all 58 cells from top to bottom (~3-5 minutes)
- **Part 1 Only**: Run cells 1-27 for preprocessing only (~1-2 minutes)
- **Part 2 Only**: Ensure Part 1 variables exist, then run cells 28-58 (~2-3 minutes)
- **Individual Steps**: Execute specific sections as needed

## üìä Notebook Features & Structure

The refactored notebook contains **58 well-organized cells** divided into two main parts:

### Part 1: IR Spectroscopy Data Preprocessing Pipeline (27 cells)

**Step 1: Import Libraries and Configuration**
   - Organized imports by category (Data Science, ML, Metrics, Scientific Computing)
   - Global configuration (plot styles, warning filters)
   - All dependencies properly imported including MinMaxScaler and savgol_filter

**Step 2: Data Loading**
   - CSV import with configurable `DATA_PATH`
   - Automatic column name cleaning
   - Dataset information display

**Step 3: Exploratory Data Analysis (EDA)**
   - Statistical summaries (info, describe)
   - Data quality checks (missing values, duplicates)
   - Spectral range analysis

**Step 4: Data Cleaning**
   - Remove missing values
   - Remove duplicates
   - Sort by wavenumber
   - Track cleaning statistics

**Step 5: Visualize Raw Spectrum**
   - Initial IR spectrum plot
   - Inverted x-axis (standard for IR spectroscopy)

**Step 6: Baseline Correction**
   - `baseline_correction_als()` function with full docstring
   - Asymmetric Least Squares (ALS) algorithm
   - Before/after comparison visualization

**Step 7: Spectral Smoothing**
   - Savitzky-Golay filter with configurable parameters
   - `WINDOW_LENGTH = 11`, `POLY_ORDER = 3`
   - Smoothing comparison visualization

**Step 8: Normalization**
   - Three normalization methods:
     - Min-Max normalization (0-1 range)
     - Standard normalization (z-score)
     - Vector normalization (L2 norm)
   - Side-by-side comparison plots

**Step 9: Spectral Resampling**
   - Cubic interpolation to uniform grid
   - Configurable `N_POINTS = 500`
   - Original vs resampled visualization

**Step 10: Feature Extraction**
   - 31 functional group regions defined in `FUNCTIONAL_GROUP_REGIONS`
   - 4 features per region: max, mean, std, area
   - Total: 124 features per spectrum

**Step 11: Save Preprocessed Data**
   - Export cleaned spectrum
   - Export resampled spectrum
   - Export extracted features

**Step 12: Preprocessing Summary**
   - Complete summary with statistics

---

### Part 2: Machine Learning for Functional Group Classification (30 cells)

**ML Step 1: Define Functional Groups**
   - 31 functional groups in `FUNCTIONAL_GROUPS` list
   - Spectral regions in `GROUP_REGIONS` dictionary

**ML Step 2: Synthetic Data Generation Functions**
   - `generate_synthetic_spectrum()` - Creates synthetic IR spectra
   - `extract_features_simple()` - Extracts 124 features per spectrum
   - Complete docstrings with parameters and returns

**ML Step 3: Generate Synthetic Dataset**
   - Configurable `N_SAMPLES = 600`
   - 1-5 functional groups per sample
   - Progress tracking during generation

**ML Step 4: Build Feature Matrix and Multi-Label Target**
   - Feature matrix: 600 samples √ó 124 features
   - Multi-label binarization with `MultiLabelBinarizer`
   - Label matrix: 600 samples √ó 31 functional groups

**ML Step 5: Visualize Functional Group Distribution**
   - Bar chart showing frequency of each functional group
   - Sorted by occurrence

**ML Step 6: Train-Test Split and Feature Scaling**
   - 80/20 train-test split
   - StandardScaler for feature normalization
   - Zero mean and unit variance

**ML Step 7: Train Individual ML Models**
   - 6 base models with optimized hyperparameters:
     - Logistic Regression
     - Random Forest (100 trees, max_depth=10)
     - Gradient Boosting (50 estimators, max_depth=5)
     - K-Nearest Neighbors (k=7)
     - Decision Tree (max_depth=15)
     - Neural Network (100-50 hidden layers)
   - Multi-output classification wrapper
   - Progress tracking with metrics

**ML Step 8: Train Ensemble Models**
   - Voting Ensemble (Soft voting)
   - AdaBoost Ensemble
   - Performance comparison with base models

**ML Step 9: Model Performance Comparison**
   - Comprehensive metrics table
   - Accuracy, F1-Micro, F1-Macro, Hamming Loss, Jaccard Score
   - Best model identification

**ML Step 10: Visualize Model Comparison**
   - Bar chart comparing all models
   - Multiple metrics side-by-side

**ML Step 11: Detailed Evaluation of Best Model**
   - Per-functional-group classification report
   - Precision, recall, F1-score for each group
   - Support statistics

**ML Step 12: Top/Bottom Performing Functional Groups**
   - Top 10 best classified groups (green bars)
   - Bottom 10 most challenging groups (red bars)
   - F1-score visualization

**ML Step 13: Save Results**
   - Model comparison CSV
   - Functional group performance CSV
   - Predictions CSV

**Final Summary**
   - Complete pipeline statistics
   - Best model highlights
   - Top 3 models ranking

## üî¨ Functional Groups Classified

The model identifies 31 different functional groups:

| Category | Examples |
|----------|----------|
| **Hydroxyl & Amine** | O-H (alcohol, carboxylic acid), N-H (primary/secondary amine) |
| **C-H Stretches** | Alkyne, Aromatic, Alkene, Alkane, Aldehyde |
| **Triple Bonds** | Nitrile (C‚â°N), Alkyne (C‚â°C), Isocyanate |
| **Carbonyl Groups** | Ester, Ketone, Aldehyde, Amide, Acid chloride, Anhydride |
| **C=C & C=N** | Aromatic, Alkene, Imine |
| **Single Bonds** | C-O (ester, alcohol, ether), C-N, S=O, C-X (halides) |

## üìà Expected Results

- **Best Model**: Typically Random Forest or Gradient Boosting
- **F1-Micro Score**: ~0.85-0.95 (varies by run due to synthetic data)
- **Accuracy**: ~0.80-0.90
- **Training Time**: < 2 minutes on standard hardware (all 8 models)

### Output Files Generated

All results are automatically saved to the project directory:

| File | Description | Size |
|------|-------------|------|
| `preprocessed_spectrum.csv` | Fully processed spectrum with all transformations | Variable |
| `resampled_spectrum.csv` | Spectrum resampled to 500 uniform points | ~15 KB |
| `extracted_features.csv` | 124 features per spectrum | ~5 KB |
| `model_comparison.csv` | Performance metrics for all 8 models | ~2 KB |
| `functional_group_performance.csv` | Per-group precision, recall, F1-score | ~4 KB |
| `predictions.csv` | Best model predictions on test set | ~10 KB |

## ‚öôÔ∏è Customization Guide

The refactored code uses **configuration constants** for easy customization:

### Preprocessing Parameters

```python
# Step 1: Data Loading
DATA_PATH = '/root/ML_Project_Classifying_Organic_Molecule/Test Data.csv'

# Step 6: Smoothing (Savitzky-Golay Filter)
WINDOW_LENGTH = 11  # Must be odd number (try 7, 11, 15)
POLY_ORDER = 3      # Polynomial order (try 2, 3, 4)

# Step 8: Spectral Resampling
N_POINTS = 500      # Number of uniform points (try 300, 500, 1000)
```

### Machine Learning Parameters

```python
# ML Step 3: Dataset Size
N_SAMPLES = 600     # Number of synthetic spectra (try 400, 600, 1000)

# ML Step 7: Model Hyperparameters
models = {
    'Random Forest': RandomForestClassifier(
        n_estimators=100,   # Number of trees (try 50, 100, 200)
        max_depth=10,       # Max tree depth (try 10, 15, 20)
        random_state=42,
        n_jobs=-1          # Use all CPU cores
    ),
    # Adjust other models similarly...
}

# ML Step 6: Train-Test Split
test_size=0.2  # 80/20 split (try 0.2, 0.25, 0.3)
```

## üõ†Ô∏è Code Quality Improvements

The notebook has been **professionally refactored** with significant improvements:

### Organization
- ‚úÖ **58 well-structured cells** with clear markdown headers
- ‚úÖ **Logical flow** from preprocessing ‚Üí feature extraction ‚Üí ML ‚Üí evaluation
- ‚úÖ **Clear separation** between Part 1 (Preprocessing) and Part 2 (ML)

### Code Quality
- ‚úÖ **Reusable functions** with complete docstrings:
  - `baseline_correction_als()` - ALS baseline correction
  - `generate_synthetic_spectrum()` - Synthetic data generation
  - `extract_features_simple()` - Feature extraction
- ‚úÖ **Configuration constants** (e.g., `DATA_PATH`, `N_POINTS`, `WINDOW_LENGTH`)
- ‚úÖ **No code duplication** - DRY principles applied
- ‚úÖ **Better variable naming** for readability

### Documentation
- ‚úÖ **Markdown cells** before each major step
- ‚úÖ **Inline comments** explaining complex operations
- ‚úÖ **Progress messages** with ‚úì symbols
- ‚úÖ **Function docstrings** with parameters and returns

### Visualizations
- ‚úÖ **Consistent styling** across all plots
- ‚úÖ **Professional formatting** (titles, labels, legends, grids)
- ‚úÖ **Appropriate colors** and alpha values
- ‚úÖ **Comparison plots** for before/after analysis

## üì¶ Dependencies

All required libraries with their specific versions are listed in `requirements.txt`:

### Main Libraries

| Library | Version | Purpose |
|---------|---------|---------|
| **numpy** | 2.3.3 | Numerical computing and array operations |
| **pandas** | 2.3.3 | Data manipulation and CSV I/O |
| **scikit-learn** | 1.7.2 | Machine learning models and metrics |
| **matplotlib** | 3.10.7 | Base plotting library |
| **seaborn** | 0.13.2 | Statistical visualizations |
| **scipy** | 1.16.2 | Scientific computing (interpolation, signal processing) |

### Key scikit-learn Components Used

- **Preprocessing**: `StandardScaler`, `MinMaxScaler`, `MultiLabelBinarizer`
- **Models**: `LogisticRegression`, `RandomForestClassifier`, `GradientBoostingClassifier`, `KNeighborsClassifier`, `DecisionTreeClassifier`, `MLPClassifier`, `AdaBoostClassifier`, `VotingClassifier`
- **Metrics**: `accuracy_score`, `f1_score`, `hamming_loss`, `jaccard_score`, `classification_report`
- **Multi-output**: `MultiOutputClassifier` for multi-label classification

### Key scipy Components Used

- `scipy.interpolate.interp1d` - Cubic interpolation for resampling
- `scipy.signal.savgol_filter` - Savitzky-Golay smoothing filter

See `requirements.txt` for the complete list including supporting dependencies.

## üìù Output Files

| File | Description |
|------|-------------|
| `preprocessed_spectrum.csv` | Fully processed spectrum with all transformations |
| `resampled_spectrum.csv` | Spectrum resampled to uniform grid |
| `extracted_features.csv` | Feature vectors for ML |
| `model_comparison.csv` | Performance metrics for all models |
| `functional_group_performance.csv` | Per-group classification metrics |
| `predictions.csv` | Best model predictions on test set |

## ü§ù Contributing

This is a course project. For suggestions or improvements:
1. Fork the repository
2. Create a feature branch
3. Make improvements
4. Submit a pull request

## üìÑ License

This project is for educational purposes.

## üë• Authors

- **CS390/CS394 Course Project**

## üôè Acknowledgments

- IR spectroscopy functional group regions based on standard literature
- Baseline correction algorithm: Asymmetric Least Squares (Eilers & Boelens, 2005)
- Savitzky-Golay filter: Original paper by Savitzky & Golay (1964)

---

**Last Updated**: October 9, 2025
**Status**: ‚úÖ Refactored and Production Ready

## Description
- Interpreting Infrared (IR) spectroscopy is crucial for identifying chemical compounds through their unique "molecular fingerprint," but traditional analysis relies heavily on human expertise, leading to potential inconsistency and error. The primary challenge is to develop a robust, systematic method to classify the functional groups of organic molecules directly from their IR spectra, thereby minimizing this human element.

- The goal is to leverage machine learning to automate and enhance this process. By training models like logistic regression, k-means clustering, and feedforward neural networks on raw spectral intensity data, the system will learn to recognize patterns associated with specific structural motifs. This approach aims to accurately and rapidly identify key functional groups, transforming molecular analysis by reducing reliance on subjective manual interpretation and significantly improving both the speed and reliability of compound identification. üöÄ
